- prepare_data.py 수정  
  DATASET = "richard-park/aihub-dataset" 로 수정  
  Huggingface에 dataset에 계속 dataset을 추가하여 tokenizer model을 개선할 것.  
  dataset 추가는 github corpus-kor-eng-2024에서 수행 중  

```  
python prepare_data.py --split=train --lang=ko --docs_to_sample=400000 --save_path=./data
python train_tokenizer.py --data_file=./data/ko.txt --save_path=./ko_tokenizer --vocab_size=32000


```  